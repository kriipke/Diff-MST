/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/cli.py:617: UserWarning: `System.configure_optimizers` will be overridden by `LightningCLI.configure_optimizers`.
  _warn(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[rank: 0] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Found 196 mix directories in ['/import/c4dm-datasets/MedleyDB_V1/V1', '/import/c4dm-datasets/MedleyDB_V2/V2'].
Found 139 mix directories in train set.
Found 101 mix directories with tracks less than 20 and more than 4.
Found 196 mix directories in ['/import/c4dm-datasets/MedleyDB_V1/V1', '/import/c4dm-datasets/MedleyDB_V2/V2'].
Found 30 mix directories in val set.
Found 20 mix directories with tracks less than 20 and more than 4.
Sanity Checking: 0it [00:00, ?it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 139/139 [00:00<00:00, 1614.53it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 1453.60it/s]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  | Name                 | Type                    | Params
-----------------------------------------------------------------
0 | model                | MixStyleTransferModel   | 26.0 M
1 | generate_mix_console | AdvancedMixConsole      | 0
2 | loss                 | MultiResolutionSTFTLoss | 0
3 | sisdr                | SISDRLoss               | 0
4 | mrstft               | MultiResolutionSTFTLoss | 0
-----------------------------------------------------------------
26.0 M    Trainable params
0         Non-trainable params
26.0 M    Total params


Sanity Checking DataLoader 0:   0%|                                                                                                                                                           | 0/2 [00:00<?, ?it/s]
Epoch 0:   0%|                                                                                                                                                                             | 0/2500 [00:00<?, ?it/s]
/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Traceback (most recent call last):
  File "/homes/ssv02/Diff-MST/main.py", line 26, in <module>
    cli_main()
  File "/homes/ssv02/Diff-MST/main.py", line 15, in cli_main
    cli = LightningCLI(
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/cli.py", line 353, in __init__
    self._run_subcommand(self.subcommand)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/cli.py", line 642, in _run_subcommand
    fn(**fn_kwargs)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 520, in fit
    call._call_and_handle_interrupt(
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 42, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 92, in launch
    return function(*args, **kwargs)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 559, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 935, in _run
    results = self._run_stage()
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 978, in _run_stage
    self.fit_loop.run()
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 201, in run
    self.advance()
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 261, in _optimizer_step
    call._call_lightning_module_hook(
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 142, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/core/module.py", line 1265, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 158, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 257, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 224, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/torch/optim/adamw.py", line 148, in step
    loss = closure()
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 101, in _wrap_closure
    closure_result = closure()
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 140, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 126, in closure
    step_output = self._step_fn()
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 308, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 288, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 329, in training_step
    return self.model(*args, **kwargs)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1110, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/pytorch_lightning/overrides/base.py", line 90, in forward
    output = self._forward_module.training_step(*inputs, **kwargs)
  File "/homes/ssv02/Diff-MST/mst/system.py", line 132, in training_step
    loss, data_dict = self.common_step(batch, batch_idx, train=True)
  File "/homes/ssv02/Diff-MST/mst/system.py", line 81, in common_step
    pred_mix_a, pred_param_dict = self(tracks_a, ref_mix_b)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/homes/ssv02/Diff-MST/mst/system.py", line 46, in forward
    return self.model(tracks, ref_mix)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/homes/ssv02/Diff-MST/mst/modules.py", line 37, in forward
    pred_mix, mix_params = self.mix_console(tracks, mix_params)
  File "/homes/ssv02/Diff-MST/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/homes/ssv02/Diff-MST/mst/modules.py", line 246, in forward
    mix = self.forward_mix_console(tracks, param_dict)
  File "/homes/ssv02/Diff-MST/mst/modules.py", line 197, in forward_mix_console
    tracks = parametric_eq(tracks, self.sample_rate, **param_dict["parametric_eq"])
  File "/homes/ssv02/Diff-MST/dasp-pytorch/dasp_pytorch/functional.py", line 219, in parametric_eq
    x_out = dasp_pytorch.signal.sosfilt_via_fsm(sos, x)
  File "/homes/ssv02/Diff-MST/dasp-pytorch/dasp_pytorch/signal.py", line 162, in sosfilt_via_fsm
    H = fft_sosfreqz(sos, n_fft=n_fft)
  File "/homes/ssv02/Diff-MST/dasp-pytorch/dasp_pytorch/signal.py", line 31, in fft_sosfreqz
    H *= fft_freqz(b, a, n_fft=n_fft)
  File "/homes/ssv02/Diff-MST/dasp-pytorch/dasp_pytorch/signal.py", line 9, in fft_freqz
    A = torch.fft.rfft(a, n_fft)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 162.00 MiB (GPU 6; 23.69 GiB total capacity; 22.00 GiB already allocated; 123.75 MiB free; 22.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF